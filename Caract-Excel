import os
import cv2
import numpy as np
import csv
from skimage.feature import greycomatrix, greycoprops
from skimage import measure
from scipy.stats import skew, kurtosis

# ------------------------ Calculo de caracteristicas y pasarlo a csv-------------------------------------
def extract_features(image):
    features = []

    # Convertir la imagen a escala de grises
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # 1. Media de intensidad en la imagen en escala de grises
    mean_intensity = np.mean(gray_image)
    features.append(mean_intensity)

    # 2. Desviación estándar de la intensidad en la imagen en escala de grises.
    std_intensity = np.std(gray_image)
    features.append(std_intensity)

    # Calcular el promedio, la desviación estándar y la varianza de cada componente de color (R, G, B)
    b, g, r = cv2.split(image)
    features.extend([np.mean(channel) for channel in (r, g, b)])
    features.extend([np.std(channel) for channel in (r, g, b)])
    features.extend([np.var(channel) for channel in (r, g, b)])

    # Convertir la imagen a espacio de color HSV
    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

    # 3. Características de color en HSV
    features.append(np.mean(hsv_image[:, :, 0]))  # Promedio de tono
    features.append(np.mean(hsv_image[:, :, 1]))  # Promedio de saturación
    features.append(np.mean(hsv_image[:, :, 2]))  # Promedio de valor

    # 4. Características de textura utilizando la matriz de co-ocurrencia de niveles de gris (GLCM)
    glcm = greycomatrix(gray_image, [1], [0, np.pi/4, np.pi/2, 3*np.pi/4], 256, symmetric=True, normed=True)
    features.append(greycoprops(glcm, 'homogeneity').mean())  # Homogeneidad
    features.append(greycoprops(glcm, 'contrast').mean())     # Contraste

    # 5. Características de forma utilizando regionprops
    props = measure.regionprops_table(measure.label(gray_image > np.mean(gray_image)), properties=('area', 'perimeter', 'solidity'))
    features.extend([np.mean(props['area']), np.mean(props['perimeter']), np.mean(props['solidity'])])

    # 6. Características estadísticas: asimetría y curtosis
    features.append(skew(gray_image, axis=None))     # Asimetría
    features.append(kurtosis(gray_image, axis=None)) # Curtosis

    return features

# Carpeta de entrada
input_folder = r"/content/drive/MyDrive/nuevos_result"

# Definir las clases
classes = ['q', 'w', 'e']

# Crear un archivo CSV para escribir las características
csv_filename = "image_features5.csv"
with open(csv_filename, 'w', newline='') as csvfile:
    csv_writer = csv.writer(csvfile)

    # Escribir el encabezado del CSV
    header = ["Class", "Mean Intensity", "Std Intensity", "Mean R", "Mean G", "Mean B", "Std R", "Std G", "Std B", "Var R", "Var G", "Var B",
              "Promedio_tono", "Promedio_saturacion", "Promedio_valor", "Homogeneidad", "Contraste", "Area", "Perimeter", "Solidity",
              "Asimetría", "Curtosis"]
    csv_writer.writerow(header)

    # Iterar sobre las clases
    for cls in classes:
        # Obtener la carpeta de la clase actual
        class_folder = os.path.join(input_folder, cls)

        # Obtener la lista de archivos en la carpeta de la clase
        file_list = os.listdir(class_folder)

        # Iterar sobre los archivos de la clase actual
        for filename in file_list:
            # Construir la ruta completa de la imagen
            image_path = os.path.join(class_folder, filename)

            # Cargar la imagen
            image = cv2.imread(image_path)

            # Extraer características de la imagen
            image_features = extract_features(image)

            # Escribir las características en el archivo CSV junto con la clase
            row = [cls] + image_features
            csv_writer.writerow(row)

print("CSV generado exitosamente:", csv_filename)

# ------------------------ Realizar la normalización -------------------------------------

import pandas as pd

# Cargar el archivo CSV
df = pd.read_csv('image_features5.csv')

# Guardar la columna 'class' antes de normalizar
clases = df['Class']

# Eliminar la columna 'class' temporalmente antes de normalizar
df = df.drop(columns=['Class'])

# Normalizar utilizando min-max scaling
df_normalized = (df - df.min()) / (df.max() - df.min())

# Agregar la columna 'class' nuevamente
df_normalized['Class'] = clases

# Guardar el archivo CSV normalizado
df_normalized.to_csv('tu_archivo_normalizado.csv', index=False)

# ------------------------ Calculo de caracteristicas normalizadas y pasarlo a csv-------------------------------------

import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical

# Cargar los datos desde el archivo CSV
data = pd.read_csv("tu_archivo_normalizado.csv")

# Dividir los datos en características (X) y etiquetas (y)
X = data.drop(columns=['Class'])
y = data['Class']

# Codificar las etiquetas en un formato adecuado para la red neuronal
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Codificar las etiquetas de clase
y_train_encoded = to_categorical(y_train, num_classes=3)
y_test_encoded = to_categorical(y_test, num_classes=3)


# Definir los rangos de parámetros
learning_rates = [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0]
momentum_values = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9]
neurons_range = list(range(3, 24))
epochs_range = [50, 100, 200, 300, 400, 500]


# Inicializar variables para el mejor accuracy y los parámetros correspondientes
best_accuracy = 0.0
best_parameters = {}

# Crear un archivo para guardar los resultados
with open('results.txt', 'w') as file:
    file.write("learning_rate\tmomentum\tcapa\tneurona\tepoca\taccuracy\n")

    # Iterar sobre los diferentes parámetros y entrenar modelos
    for learning_rate in learning_rates:
        for momentum in momentum_values:
            for neurons in neurons_range:
                for epochs in epochs_range:
                    # Definir el modelo de red neuronal
                    model = Sequential([
                        Dense(neurons, input_shape=(21,), activation='relu'),
                        Dense(3, activation='softmax')
                    ])

                    # Compilar el modelo
                    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum)
                    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

                    # Entrenar el modelo
                    model.fit(X_train, y_train_encoded, epochs=epochs, batch_size=16, verbose=0)

                    # Evaluar el modelo
                    _, accuracy = model.evaluate(X_test, y_test_encoded, verbose=0)

                    # Escribir los resultados en el archivo
                    file.write(f"{learning_rate}\t{momentum}\t1\t{neurons}\t{epochs}\t{accuracy}\n")

                    # Actualizar el mejor accuracy y los parámetros correspondientes si se encuentra un accuracy mejor
                    if accuracy > best_accuracy:
                        best_accuracy = accuracy
                        best_parameters['learning_rate'] = learning_rate
                        best_parameters['momentum'] = momentum
                        best_parameters['neurons'] = neurons
                        best_parameters['epochs'] = epochs

                    # Imprimir los resultados
                    print(f"Learning rate: {learning_rate}, Momentum: {momentum}, Capa: 1, Neurona: {neurons}, Época: {epochs}, Accuracy: {accuracy}")

# Imprimir los mejores parámetros y el mejor accuracy
print("Mejor Accuracy:", best_accuracy)
print("Mejores Parámetros:", best_parameters)

